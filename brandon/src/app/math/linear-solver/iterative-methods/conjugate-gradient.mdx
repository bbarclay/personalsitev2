---
title: "Conjugate Gradient Method"
description: "An efficient iterative method for solving positive-definite linear systems"
---

import { Callout } from 'nextra/components'
import ConjugateGradientDemo from '../../../components/demos/ConjugateGradientDemo'

# Conjugate Gradient Method

<Callout type="info">
  The Conjugate Gradient method is an iterative algorithm for solving systems of linear equations where the coefficient matrix is symmetric and positive-definite.
</Callout>

## Introduction

The Conjugate Gradient (CG) method, developed by Magnus Hestenes and Eduard Stiefel in 1952, is one of the most effective iterative methods for solving large, sparse systems of linear equations where the coefficient matrix is symmetric and positive-definite.

The key insight of the CG method is that finding the solution to $Ax = b$ is equivalent to minimizing the quadratic function:

$$f(x) = \frac{1}{2}x^TAx - b^Tx + c$$

## Mathematical Foundation

The CG method generates a sequence of approximate solutions $x_k$ that converge to the exact solution. Each approximation is improved along a search direction $p_k$ that is A-conjugate to all previous directions, meaning $p_i^T A p_j = 0$ for $i \neq j$.

## Algorithm

1. Initialize:
   - Set initial guess $x_0$
   - Compute initial residual $r_0 = b - Ax_0$
   - Set initial search direction $p_0 = r_0$

2. For $k = 0, 1, 2, \ldots$ until convergence:
   - Compute step length: $\alpha_k = \frac{r_k^T r_k}{p_k^T A p_k}$
   - Update solution: $x_{k+1} = x_k + \alpha_k p_k$
   - Update residual: $r_{k+1} = r_k - \alpha_k A p_k$
   - Check for convergence: If $\|r_{k+1}\| < \text{tolerance}$ then stop
   - Compute direction update coefficient: $\beta_k = \frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$
   - Update search direction: $p_{k+1} = r_{k+1} + \beta_k p_k$

## Implementation

Here's a Python implementation of the Conjugate Gradient method:

```python
import numpy as np

def conjugate_gradient(A, b, x0=None, tol=1e-6, max_iter=1000):
    n = len(b)
    if x0 is None:
        x = np.zeros(n)
    else:
        x = x0.copy()
    
    r = b - A @ x
    p = r.copy()
    r_norm_sq = r @ r
    
    for i in range(max_iter):
        Ap = A @ p
        alpha = r_norm_sq / (p @ Ap)
        x = x + alpha * p
        r = r - alpha * Ap
        
        r_norm_sq_new = r @ r
        if np.sqrt(r_norm_sq_new) < tol:
            print(f"Converged in {i+1} iterations")
            break
            
        beta = r_norm_sq_new / r_norm_sq
        p = r + beta * p
        r_norm_sq = r_norm_sq_new
    
    return x
```

## Convergence Properties

The Conjugate Gradient method has several remarkable convergence properties:

1. In exact arithmetic, the method converges in at most $n$ iterations for an $n \times n$ matrix.
2. In practice with finite precision, convergence is often much faster, especially for well-conditioned matrices.
3. The convergence rate depends on the condition number $\kappa(A)$ and the eigenvalue distribution of $A$.

## Preconditioned Conjugate Gradient

For ill-conditioned systems, preconditioning can significantly improve convergence:

<Callout>
  Preconditioning transforms the original system $Ax = b$ into an equivalent system with better convergence properties.
</Callout>

The preconditioned system is:
$$M^{-1}Ax = M^{-1}b$$

where $M$ is chosen such that $M^{-1}A$ has a more favorable eigenvalue distribution than $A$.

## Applications

The Conjugate Gradient method is particularly effective for:

- Structural analysis with finite element methods
- Image reconstruction and processing
- Machine learning algorithms
- Partial differential equation solvers
- Network analysis

## Advantages and Limitations

### Advantages
- Memory efficient (only requires storing a few vectors)
- Optimal convergence for symmetric positive-definite systems
- Well-suited for sparse matrices

### Limitations
- Requires symmetric positive-definite coefficient matrix
- Sensitive to round-off errors
- May converge slowly for ill-conditioned systems without preconditioning

## Interactive Demonstration

<ConjugateGradientDemo />

## Related Methods

- [Biconjugate Gradient Method](/linear-solver/iterative-methods/biconjugate-gradient)
- [GMRES Method](/linear-solver/iterative-methods/gmres)
- [Preconditioners](/linear-solver/concepts/preconditioners)
